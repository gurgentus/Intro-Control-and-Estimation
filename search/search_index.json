{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Control Theory and State Estimation","text":"<p>Lecture notes for the class I taught on Control Theory and Estimation at Ohio University in Spring 2017.</p> <p>This tutorial contains a set of notes I created for a special topics course in control theory and state estimation for applied math majors. Control theory is a perfect \u2018capstone\u2019 course for advance undergraduates as it combines rigorous mathematics of optimization, differential equations, dynamical systems, linear algebra, calculus of variations, probability, and other more theoretical areas of mathematics with practical use of computer programming, numerical methods and engineering applications. As such, the goal is twofold, first to introduce the more theory oriented students to the beautiful world of engineering applications in control theory, a perfect example of the use of the higher level mathematics in the `real world\u2019, and second, to give a sound theoretical footing to those students that are more applied and engineering oriented.</p> <p>In parallel, I am developing a Cloud based interactive Controls and Simulation Toolbox that can be accessed at:</p> <p>Cloud Control Toolbox (CCST)</p> <p>Warning: These notes are a work in progress.  Realistically, due to time constraints end of Module 3 and Module 4 would be taught as a second semester of the course.  </p>"},{"location":"#references","title":"References:","text":"<p>Optimal Control and Estimation, Robert F. Stengel</p> <p>Calculus of Variations and Optimal Control Theory, Daniel Liberzon</p> <p>Optimal Control Theory for Applications, David G. Hull</p> <p>Applied Optimal Control, Arthur E. Bryson, Jr and Yu-Chi Ho</p> <p>\"An optimal guidance law for planetary landing\", Christopher D'Souza, paper, Guidance, Navigation, and Control Conference</p> <p>Probabilistic Robotics, Sebastian Thrun, Wolfram Burgard, and Dieter Fox</p> <p>A Course in Robust Control Theory: a convex approach, Geir E. Dullerud and Fernando G. Paganini</p> <p>Robust and Adaptive Control with Aerospace Applications, Eugene Lavretsky and Kevin A. Wise</p> <p>\u0422\u0435\u043e\u0440\u0438\u044f \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f, \u0413\u043e\u043b\u044c\u0434\u0444\u0430\u0440\u0431 \u041b. \u0421, \u0411\u0430\u043b\u0442\u0440\u0443\u0448\u0435\u0432\u0438\u0447 \u0410. \u0412., \u041d\u0435\u0442\u0443\u0448\u0438\u043b \u0410. \u0412. \u0438 \u0434\u0440.</p>"},{"location":"#model-references","title":"Model References:","text":"<p>2D Car model - I found the original system without the derivation in \"Optimal Control and Estimation\" by Robert F. Stengel.  After not finding a satisfying derivation of the equations in other references I derived it myself as shown in the first section below.  Interestingly, the right-hand side of the rate of orientation change turned out to be different from the original model in Stengel.</p> <p>Landing Spacecraft - Christopher D'Souza. \"An optimal guidance law for planetary landing\", Guidance, Navigation, and Control Conference, http://dx.doi.org/10.2514/6.1997-3709.  Thanks to Ronald Sostaric, NASA, for the suggestion of this paper, which is also used for the Module 2 project.</p>"},{"location":"#table-of-context","title":"Table of Context","text":""},{"location":"#module-1-classical-control","title":"Module 1 - Classical Control","text":"<p> Introduction and 2D Car Model </p> <p> Frequency Domain Analysis </p> <p> Controller Design Basics </p> <p> Root Locus Analysis </p> <p> Our First Controller </p> <p> Extra: Simulating the World </p> <p> Project 1 - Adaptive Cruise Control </p>"},{"location":"#module-2-optimal-control","title":"Module 2 - Optimal Control","text":"<p> Introduction </p> <p> Calculus of Variations </p> <p> Optimal Control using Calculus of Variations </p> <p> Maximum Principle </p> <p> Hamilton-Jacobi-Bellman Equation </p> <p> The Linear Quadratic Regulator </p> <p> Project 2 - Planetary Landing Guidance </p>"},{"location":"#module-3-optimal-state-estimation","title":"Module 3 - Optimal State Estimation","text":"<p> Introduction </p> <p> Simple Estimator </p> <p> Gaussian Filters </p> <p> Particle Filters </p> <p> Extra: Adaptive Filtering </p>"},{"location":"#module-4-advanced-topics-robust-and-adaptive-control","title":"Module 4 - Advanced Topics, Robust and Adaptive Control","text":"<p> Controllability </p> <p> Eigenvalue Assignment </p> <p> Observability </p> <p> H2 Optimal Control </p> <p> Model Reference Adaptive Control </p> <p> Robust Adaptive Control </p>"},{"location":"cloud-control-toolbox/","title":"Cloud Control and Simulation Toolbox (CCST)","text":"<p>An interactive web-based platform for control theory simulation, visualization, and learning.</p> <p>\ud83d\ude80 Launch Cloud Control Toolbox</p>"},{"location":"cloud-control-toolbox/#features","title":"Features","text":"<p>The Cloud Control and Simulation Toolbox provides:</p> <ul> <li>Octave Integration - Run control algorithms in the browser</li> <li>Path Planning - Interactive path planning algorithms and visualization</li> <li>Orbital Mechanics - Spacecraft trajectory simulation and analysis</li> <li>Real-time Visualization - Dynamic plots powered by Bokeh</li> <li>Interactive Terminal - Command-line interface for advanced users</li> <li>Root Locus Analysis - Interactive root locus plotting and design</li> <li>State Space Simulation - Simulate and visualize state space systems</li> <li>PID Controller Design - Tune and test PID controllers</li> </ul>"},{"location":"cloud-control-toolbox/#usage","title":"Usage","text":"<p>Click the launch button above to access the full interactive toolbox, or use the embedded version below. Everything runs in your browser - no installation or downloads required!</p> <p>The toolbox is designed to complement the course material and provide hands-on experience with the concepts covered in the lectures.</p>"},{"location":"cloud-control-toolbox/#interactive-toolbox","title":"Interactive Toolbox","text":"<p>Full Screen Mode</p> <p>Click the launch button above or use your browser's fullscreen option for the best experience.</p>"},{"location":"module1/controller-design-basics/","title":"Controller Design Basics","text":"<p>Mathematical connections: differential equations, Laplace transform</p> <p>We are now ready to analyze our first closed-loop system. As we saw in the last lecture it can be very convenient to represent Input/Output relations using the transfer function approach. In this way we can reduce the dynamics to a relation</p> \\[ Y(s) = T(s) U(s), \\] <p>between the Laplace Transforms of the control and the output. We will call \\(T(s)\\) the plant transfer function. One of the simplest ways to construct a closed-loop feedback control is to take the difference between a given 'desired' output \\(y_d(t)\\) and the actual output \\(y(t)\\) and feed a constant multiple of this 'error' in output back as the control. For nonzero \\(y_d(t)\\) this type of control is called 'tracking control'.</p> <p>At this point we are still concerned with SISO (single input, single output) systems where the transfer function is a scalar, so you can easily verify that in the Laplace Transform world this corresponds to choosing \\(U(s) = k (Y_d(s) - Y(s))\\) resulting in the new relation:</p> \\[ Y(s) = k T(s) (Y_d(s) - Y(s)) \\] <p>or solving for \\(Y(s)\\):</p> \\[ Y(s) = \\frac{k T(s)}{1 + k T(s)} Y_d(s) \\] <p>We now have a new transfer function \\begin{equation} \\label{propcont} T_c(s) = \\frac{k T(s)}{1 + k T(s)} \\end{equation} that gives us an input/output relation between the command signal (desired \\(y_d\\)) and the actual output.</p> <p>You can imagine representing a more complicated relation between the tracking error \\(y_d(t) - y(t)\\) and the control \\(u(t)\\) (see for instance examples in the last lecture) by</p> \\[ U(s) = K(s) (Y_d(s) - Y(s)) \\] <p>The dynamical system with the transfer function \\(K(s)\\) is called the controller. You can easily verify that in this case the relation \\eqref{propcont} has a more general form</p> \\[ T_c(s) = \\frac{K(s) T(s)}{1 + K(s) T(s)} \\] <p>Before delving into this general of a system, let's explore this type of tracking control system further on a simpler example. Let's assume our original system that we want to control (the plant) is given by the second-order differential equation \\begin{equation} \\label{secondord} m \\ddot{x} + \\beta \\dot{x} + \\kappa x = u. \\end{equation} This should be very familiar from an Elementary Differential Equations course as a model for mass-spring system in which \\(x\\) represents the position from the equilibrium at time \\(t \\in [0, \\infty)\\) of the object of mass \\(m\\) attached to a spring with spring constant \\(\\kappa\\) moving in a medium offering a resistance that is \\(\\beta\\) times the instantaneous velocity under an external force \\(u\\). We will see that this equation is also very important in many other applications as oftentimes it approximates dynamics that are much more complex (e.g. an airplane flight).</p> <p>Assuming our output is simply \\(y(t) = x(t)\\), and taking the Laplace Transform of both sides and assuming zero initial conditions you can verify that the open-loop input/output relation is: \\begin{equation} \\label{siso} Y(s) = \\frac{1}{m s^2 + \\beta s + \\kappa} U(s). \\end{equation} If it's been a while since differential equations here's a refresher on Laplace Transforms of derivatives -&gt; Laplace Transforms</p> <p>First we investigate the problem as an open-loop control. At this point do the following as homework:</p>   1. Write the second order scalar differential equation \\eqref{secondord} above as a first order system (recall that you want to define a new state variable as the derivative of $x$). You may abuse notation and use $x = [x, \\dot{x}]^T$ as the name of the resulting vector. Then the resulting system should be in the form from last lecture.     \\[    \\dot{x}(t) = F x(t) + G u(t),    \\]    \\[    y(t) = x(t).    \\]    So you just have to find $F$ and $G$.     For nonzero $u$ this is a nonhomogeneous system. What are the eigenvalues and eigenvectors of $F$? For this it will be helpful to define the natural frequency $\\omega := \\sqrt{\\frac{\\kappa}{m}}$ and the damping ratio $\\xi := \\frac{\\beta}{2 m \\omega}$ and consider several cases depending on the values of the parameters. Find the fundamental matrix $\\Phi(t)$ and the solution of the corresponding homogeneous system in each of the cases. You are free to use any differential equations textbook or web resource as a reference.     Write the 'Variation of Parameters' formula for the nonhomogeneous equation (this is the integral formula for the solution of nonhomogeneous system, do NOT derive this formula!).  2. Assume initial conditions are $x(0) = \\dot{x}(0) = 0$.     Go back to the system in the previous part. Take the Laplace Transform of the system and show that you get the same relation \\eqref{siso} whether you work with the second order scalar equation or the corresponding first order system.     Verify that it is also the same as was derived in the last lecture, i.e.:     $$    T(s) = [H (sI - F)^{-1} G]    $$    In this case $H= [1,0]^T$, since the output $y(t) = x(t)$.     Assume $u = u_0$ is constant. Here we consider the equation only on the interval $t \\in [0, \\infty)$. We could consider $t \\in (-\\infty, \\infty)$ and define $u$ to be a unit step function (i.e. equal to $0$ for $t &lt; 0$ and $u_0$ for $t \\ge 0$). Such formulation will prove to be useful later, but don't worry about it for now. (Hint: in either case the Laplace Transform is $u_0/s$).     What is the steady state in this case? (Note you can use the system to find this, but also the scalar equation \\eqref{secondord} gives this immediately since the steady state corresponds to the derivatives equal to zero).     Write down the expression for $Y(s)$ in this case. Write the expression for $Y(s)$ in terms of the parameters $\\omega$ and $\\xi$.  3. Solve the equation by finding the inverse Laplace Transform $y(t)$. This is the most technical step. You'll have to do a partial fraction decomposition and consider several cases depending on the value of $\\xi$. There should be many parallels between this and the fundamental matrix analysis from the first part. Again you are free to use any differential equations textbook or web resource as a reference.     Note that the zeros of the denominator of the transfer function which you had to calculate as part of the partial fraction decomposition are the same as the eigenvalues of the matrix $F$ from the first step.     From the abstract point of view, this is simply the consequence of the inversion formula of a matrix in terms of it's adjoint matrix (matrix of cofactors)    $$    T(s) = [H (sI - F)^{-1} G] = \\frac{H Adj(sI - F) G}{det(sI - F)}    $$    As you can see the condition that the denominator equals $0$ is exactly the same condition as $s$ being an eigenvalue of $F$.  4. What happens in each of the cases when $t \\rightarrow \\infty$? How does this behavior depend on the zeros of the denominator of the transfer function $T(s)$ (presumably you found those when doing the partial fraction decomposition).  5. Choose $\\kappa = 1$ and $\\omega = 2 \\pi$ and plot the solution for various values of $\\xi$. Make sure you select at least one value from each of the cases.   <p>As you can see even for a simple second order constant coefficient differential equation there could be a lot of different behaviors of the solution. If the idea is to use a control to drive the output \\(y(t)\\) to a steady state, the success (and performance) of such endeavor depends critically on the parameters of the system, which might be fixed.</p> <p>Here is a natural question then. If the behavior is not what we want, can a feedback mechanism improve the situation? Say we want to 'drive' \\(y(t)\\) to the value \\(y_0\\). Going back to \\eqref{propcont}, you can imagine the simplest type of command signal \\(y_d\\) to be a constant command \\(y_d(t) = y_0\\). Using the corresponding Laplace Transform \\(Y_d(s) = y_0/s\\) in \\eqref{propcont} results in</p> \\[ Y(s) = \\frac{k u_0 T(s)}{s (1 + k T(s))} = T_c(s) Y_d(s), \\] <p>where</p> \\[ T_c(s) = \\frac{k T(s)}{(1 + k T(s))} \\] <p>Let us now write the transfer function \\(T(s)\\) as</p> \\[ T(s) = N(s)/D(s), \\] <p>where \\(N(s)\\) and \\(D(s)\\) are polynomials of degree \\(m\\) and \\(n\\) \\((m \\le n)\\) respectively representing the numerator and the denominator. Then some algebra gives</p> \\[ T_c(s) = \\frac{k N(s)}{D(s) + k N(s)} \\] <p>as before.</p> <p>As you saw in the exercise the stability and performance characteristics of the plant depend on the sign of the real parts of zeros of the denominator \\(D(s)\\) (note that these were also the eigenvalues of the original system). In particular to reach the desired steady state it must be true that the real parts of those zeros are negative). By introducing the feedback control we changed the problem into studying the zeros of the new denominator \\(D(s) + k N(s)\\), so the question becomes whether we can choose \\(k\\) to have the poles (zeros of the denominator) of the new transfer function have negative real parts. Or more generally how do the roots of</p> \\[ D(s) + k N(s) = 0 \\] <p>relate to the roots of</p> \\[ D(s) = 0. \\] <p>This is the question of the next lecture.</p>"},{"location":"module1/frequency-domain-analysis/","title":"Frequency Domain Analysis","text":"<p>Mathematical connections: differential equations, Laplace transform.</p> <p>This week we concentrate on systems of differential equations of the form</p> \\[\\begin{equation} \\dot{x}(t) = F(t) x(t) + G(t) u(t) + L(t) w(t), \\label{eq:dyneq} \\end{equation}\\] <p>where \\(u\\) represents controllable inputs and \\(w\\) represents uncontrollable inputs (disturbances).</p> <p>As we saw last week, such linear systems usually arise by the process of linearization, where the corresponding state, input, and disturbance vectors are written as perturbations from some 'simple' state, input, and disturbance values:</p> \\[ x_0 + x(t), u_0 + u(t), \\mbox{ and } w_0 + w(t). \\] <p>We will come back to the idea of linearization in later weeks, so for now there is no danger in assuming that \\eqref{eq:dyneq} describes our model precisely.</p> <p>Our goal is to study the dynamics in the form of an abstract first order linear differential equation</p> \\[\\begin{equation} \\label{eq:abssys} \\dot{x}(t) = A(t) x(t) + e(t), \\end{equation}\\] <p>where the equation is homogeneous if \\(e \\equiv 0\\).</p>"},{"location":"module1/frequency-domain-analysis/#types-of-control","title":"Types of Control","text":"<p>We may study various types of controls:</p>"},{"location":"module1/frequency-domain-analysis/#open-loop-control","title":"Open-Loop Control","text":"<p>In an open-loop control \\(u(t)\\) is a predetermined function and so the system can be written as</p> \\[ \\dot{x} = A(t) x + e(t), \\] <p>where \\(A(t) = F(t)\\) and \\(e(t) = G(t) u(t) + L(t) w(t)\\). Even without disturbances the equation is nonhomogeneous.</p>"},{"location":"module1/frequency-domain-analysis/#closed-loop-feedback-control","title":"Closed-Loop Feedback Control","text":"<p>Systems with closed-loop feedback control may assume the control can be written as:</p> \\[ u(t) = -C(t) x(t), \\] <p>and so our system can be written as:</p> \\[ \\dot{x} = A(t) x + e(t), \\] <p>where \\(A(t) := F(t) - G(t) C(t)\\) and \\(e(t) := L(t) w(t)\\). Note if there are no disturbances we get a homogeneous system.</p> <p>Intuitively think about designing control actions for flying an airplane, it would probably produce a more 'stable' situation if your control somehow reacts to the state of the airplane, as opposed to predetermining all of the controls on the ground before taking off (even if you calculated that those controls should be optimal, small unpredictable disturbances sure to occur in the air could make those controls useless).</p>"},{"location":"module1/frequency-domain-analysis/#dynamic-compensation","title":"Dynamic Compensation","text":"<p>In addition one could add dynamic compensation to the control feedback law:</p> \\[ u(t) = -C_1(t) x(t) - C_2(t) \\int_0^t H_\\xi x(\\tau) d \\tau, \\] <p>i.e. the control \\(u\\) depends not just on the current state, but on the contribution from all previous states. Here \\(H_\\xi\\) is an identity matrix with some of the rows removed (if the corresponding element of the state should not have an effect).</p> <p>Can we still put this into the framework of \\eqref{eq:abssys}? Let's define \\(\\xi(t)\\) to be the solution of</p> \\[ \\dot{\\xi}(t) = H_\\xi x(t), \\quad \\xi(0) = 0. \\] <p>Then by the Fundamental Theorem of Calculus</p> \\[ u(t) = -C_1(t) x(t) - C_2(t) \\xi(t) \\] <p>and the dynamics can be described by</p> \\[ \\begin{bmatrix} \\dot{x}(t) \\\\ \\dot{\\xi}(t) \\end{bmatrix} = \\begin{bmatrix} F(t) - G(t) C_1(t) &amp; -G(t) C_2(t) \\\\ H_\\xi &amp; 0 \\end{bmatrix} \\begin{bmatrix} x(t) \\\\ \\xi(t) \\end{bmatrix} + \\begin{bmatrix} L(t) w(t) \\\\ 0 \\end{bmatrix}, \\] <p>which redefining \\(x\\) to be the above vector can again be written in the form</p> \\[ \\dot{x} = A(t) x + e(t). \\] <p>Again without disturbances \\(w\\) the system is homogeneous.</p>"},{"location":"module1/frequency-domain-analysis/#actuator-dynamics","title":"Actuator Dynamics","text":"<p>Finally, the choice of control could involve solving a differential equations on its own. Such actuator dynamics can take the form</p> \\[ \\dot{u}(t) = F_C(t) u(t), \\] <p>for a given \\(F_C(t)\\). Here we can again write the equation in the general form (RS 2.5-19)</p> \\[ \\begin{bmatrix} \\dot{x}(t) \\\\ \\dot{u}(t) \\end{bmatrix} = \\begin{bmatrix} F(t) &amp; G(t) \\\\ 0 &amp; F_C(t) \\end{bmatrix} \\begin{bmatrix} x(t) \\\\ u(t) \\end{bmatrix} + \\begin{bmatrix} L(t) w(t) \\\\ 0 \\end{bmatrix} \\]"},{"location":"module1/frequency-domain-analysis/#laplace-transform-and-frequency-domain-methods","title":"Laplace Transform and Frequency Domain Methods","text":"<p>Hopefully at this point you are convinced that there are various way to design our controls and a pressing question is how these choices can be designed to drive the state of our model to the behavior we want. One complication of this analysis is the fact that the relations between the control and the state are through differential equations. As we will see in the remainder of this week, the Laplace Transform which you doubtless encountered in an elementary differential equations course is extremely helpful in this context. Below I describe the 'Frequency Domain Methods' that utilize this marvelous invention of Pierre-Simon Laplace.</p> <p>We'll come back to the feedback controls soon enough, but for now let's return to \\eqref{eq:dyneq} as is and assume in addition that the matrices \\(F\\) and \\(G\\) do not depend on \\(t\\) and there is no uncontrollable input. Recall that the Laplace Transform of the resulting equation</p> \\[ \\dot{x}(t) = F x(t) + G u(t) \\] <p>is given by</p> \\[ s X(s) = F X(s) + G U(s) + x(0). \\] <p>We also assume that we have an output vector</p> \\[ y(t) = H x(t), \\] <p>with the corresponding equation in the 'frequency domain':</p> \\[\\begin{equation} \\label{eq:siso} Y(s) = H X(s). \\end{equation}\\] <p>If you have only seen Laplace Transforms applied to scalar equations you should convince yourself that the formulas above still make sense in the vector setting, after all you are simply taking the Laplace Transform of all equations in your system.</p>"},{"location":"module1/frequency-domain-analysis/#transfer-functions","title":"Transfer Functions","text":"<p>Assuming \\(x(0) = 0\\) and using a little linear algebra magic we can write the input-output relation in the frequency domain as</p> \\[ (sI - F) X(s) = G U(s), \\] <p>or</p> \\[ X(s) = (sI - F)^{-1} G U(s). \\] <p>Similarly,</p> \\[ Y(s) = H X(s) = [H (sI - F)^{-1} G] U(s). \\] <p>In the frequency domain the relation between the control and the state or the output is simply represented by a multiplication by an \\(s\\)-dependent matrix, an ugly matrix, \\(H (sI - F)^{-1} G\\) but still just a matrix. We will call it the transfer function.</p> <p>Finally, we make one more assumption destroying large part of our generalization only to be recovered later in the semester. That is we assume that the control \\(u(t)\\) is a single scalar and so is the output \\(y(t)\\). In other words the ugly matrix \\(H (sI - F)^{-1} G\\) relating the corresponding Laplace Transforms is nothing other than a scalar. Don't relax too much though \\(H\\), \\(I\\), \\(F\\), and \\(G\\) are still matrices (or at best vectors), as I never said that the state \\(x(t)\\) is a scalar, that would make things really too simple and not as interesting. So \\(x\\) is still an n-dimensional vector and the dynamics equation is still a system, all we are saying is that it is controlled by one input and produces one output. You should check that it then makes sense that \\(G\\) is \\(n \\times 1\\), \\(F\\) is \\(n \\times n\\) and \\(H\\) to be \\(1 \\times n\\).</p>"},{"location":"module1/frequency-domain-analysis/#exercise-applying-to-the-car-model","title":"Exercise: Applying to the Car Model","text":"Time to get your hands dirty. Take the linearization of the simple car model from last week. We'll need to get rid of some of the controls to fit it into our assumptions. I will suggest assuming a constant thrust $T = T_0$ and leaving the steering to be our only control $u = \\delta w$. We will also have to introduce the output equation $y(t) = H x(t)$ for this model. Our state $x$ consisted of things like velocity, orientation, the position coordinates, and mass. Let's take the orientation $\\xi$ as our output. Can you see what $H$ would have to be to provide this information to $y$ on the proverbial golden plate? How about $H = [0, 1, 0, 0, 0]$ since the orientation $\\xi$ appeared as the second component of $x$?  At this point you should be able to turn in an assignment with a nonlinear system of differential equations describing the simple car model, the linearized version done last week (written in the abstract form of \\eqref{eq:dyneq} and \\eqref{eq:siso}, with the expressions for $F$, $G$,and $H$ given separately, and an expression in the form:  $$ Y(s) = T(s) U(s) $$  relating the single input to single output in the frequency domain. Again, leave this expression in the form above, just add what $T(s)$ is separately."},{"location":"module1/introduction/","title":"Introduction and 2D Car Model","text":"<p>Control theory deals with studying and modifying the behavior of dynamical systems with inputs and outputs. The actual system is usually referred to as plant and the system that decides on the input (possibly based on the observed output) is a controller. Our goal here is not to spend a lot of time on the full history or overview of this fascinating field that started with practical engineering problems and slowly gained its rigid mathematical structure, at the same time being pushed further and further by more and more complicated problems of engineering.</p> <p>Our study of control theory will be largely mathematical with examples drawn principally from problems in autonomous vehicle control and air/spacecraft control. Of course, one of the main benefits of concentrating on the mathematics is that everything you learn can be ported to studying problems in unrelated fields ranging from biology to social sciences to economics.</p> <p>With that let's delve straight into our first plant. Most of Module 1 will use a relatively simple model of a three degree of freedom car culminating with a project consisting of designing an adaptive cruise control for an autonomous vehicle. (Preview: In Module 2 we will introduce a model for landing a spacecraft on a planet while minimizing the acceleration, stay tuned...)</p>"},{"location":"module1/introduction/#the-2d-car-model","title":"The 2D Car Model","text":"<p>For now we remain on the ground, in fact assuming that this ground is completely flat, and describe the car with the following state variables: the position of its center of mass in the Cartesian coordinates, \\(x\\), \\(y\\), its angle of orientation, \\(\\xi\\), its velocity, \\(v\\), and its mass, \\(m\\). The inputs that can be controlled will be the amount of thrust applied, \\(\\delta T\\), the amount of breaking, \\(\\delta B\\), and the angle by which the front wheels are turned, \\(\\delta w\\). Below is a diagram summarizing this variables.</p> <p>TODO</p> <p>ADD PICTURE</p>"},{"location":"module1/introduction/#deriving-the-dynamics","title":"Deriving the Dynamics","text":"<p>The next step is to derive a differential equation that describes the dynamics. We'll start easy. The velocity components in the \\(x\\) and \\(y\\) directions can be simply obtained as vector projections of \\(v\\) onto the coordinate axis using trigonometry,</p> \\[ \\dot{x} = v \\cos(\\xi), \\] \\[ \\dot{y} = v \\sin(\\xi). \\] <p>We'll also assume that the mass changes as a function of velocity and the throttle, i.e.</p> \\[ \\dot{m} = g(v, \\delta T) \\] <p>This can be specified more explicitly depending on the properties of the actual car.</p>"},{"location":"module1/introduction/#applying-newtons-second-law","title":"Applying Newton's Second Law","text":"<p>Now for the more complicated pieces. As is often the case with these kinds of problems, our best friend is Newton's Second law, \\(F = m a = m \\dot{v}\\), written in terms of the velocity \\(v\\). What are the different forces acting on the car? To keep things a bit general we will assume that the total thrust provided by the car is \\(T(v, \\delta T, \\delta B)\\), a function of the current velocity, as well as throttle and break amount. We also assume that the component of \\(T\\) perpendicular to the tire has an equal friction force in the opposite direction to cancel it, leaving the parallel component, \\(T \\cos(\\delta w)\\) as the net force (see the picture for the force diagram). In turn, this force has components \\(T \\cos(\\delta w) \\cos(\\delta w)\\) in the direction of forward acceleration, \\(\\dot{v}\\) and \\(T \\cos(\\delta w) \\sin(\\delta w)\\) in the tangential direction. It follows that,</p> \\[ m \\dot{v} = T \\cos^2 (\\delta w) - D(v), \\] <p>where the last term is the drag due to air resistance, often approximated with, \\(C_D \\rho v^2 S/2\\), where \\(C_D\\) is the drag coefficient, \\(\\rho\\) is the density of air, \\(S\\) is the area of the surface. We'll keep this as \\(D(v)\\) to keep things general.</p>"},{"location":"module1/introduction/#angular-velocity","title":"Angular Velocity","text":"<p>The effect of the tangential force \\(T \\cos(\\delta w) \\sin(\\delta w)\\) is slightly more technical to derive using Newton's Law. To keep things relatively simple, we'll use a slightly different argument to obtain an expression for the angular velocity \\(\\dot{\\xi}\\). Assume that in time \\(\\Delta t\\) the base of the car moved \\(v \\Delta t\\) in original direction, \\(\\xi\\), while the orientation \\(\\xi\\) changed by \\(\\Delta \\xi\\) as shown in the zoomed in picture below</p> <p>TODO</p> <p>ADD PICTURE</p> <p>Then, using the law of sines from trigonometry yields</p> \\[ \\frac{l}{\\sin(180-\\delta w)} = \\frac{l - v \\Delta t}{\\sin(\\delta w - \\Delta \\xi)}, \\] <p>or after simplification</p> \\[\\begin{equation} \\label{m2} l \\sin(\\delta w - \\Delta \\xi) = l \\sin(\\delta w) - v \\Delta t \\sin(\\delta w). \\end{equation}\\] <p>Note that by the mean value theorem \\(\\sin(\\delta w) - \\sin(\\delta w - \\Delta \\xi) = \\cos(\\delta w_*) \\Delta \\xi\\) for some value of \\(\\delta w_*\\) between \\(\\delta w - \\Delta \\xi\\) and \\(\\delta w\\). Substituting the expression for \\(\\sin(\\delta w - \\Delta \\xi)\\) into \\eqref{m2} results in,</p> \\[ l \\sin(\\delta w) - l \\Delta \\xi \\cos(\\delta w_*) = l \\sin(\\delta w) - v \\Delta t \\sin(\\delta w), \\] <p>or after simplification</p> \\[ \\frac{\\Delta \\xi}{\\Delta t} = \\frac{v}{l} \\frac{\\sin(\\delta w)}{\\cos(\\delta w_*)} \\] <p>Finally, taking the limit as \\(\\Delta t \\rightarrow 0\\) (and consequently \\(\\delta w_* \\rightarrow \\delta w\\)), we have</p> \\[ \\frac{d \\xi}{ dt} = \\frac{v}{l} \\tan(\\delta w). \\]"},{"location":"module1/introduction/#system-summary","title":"System Summary","text":"Let's summarize our derived system of differential equations below,  $$ \\begin{array}{ccc} \\dot{v} &amp; = &amp; \\frac{T(v, \\delta T, \\delta B) \\cos^2 (\\delta w) - D(v)}{m}, \\\\ \\dot{\\xi} &amp; = &amp; \\frac{v}{l} \\tan(\\delta w), \\\\ \\dot{x} &amp; = &amp; v \\cos(\\xi), \\\\ \\dot{y} &amp; = &amp; v \\sin(\\xi), \\\\ \\dot{m} &amp; = &amp; g(v, \\delta T). \\end{array} $$"},{"location":"module1/our-first-controller/","title":"First Controller","text":"<p>Let's take a simplified version of our car model. The dynamic and output equations are</p> \\[ \\dot{s} = v, \\] \\[ \\dot{v} = u/m_0, \\] \\[ y = s. \\] <p>Here we chose to have the total traveled distance, \\(s\\), as the output instead of velocity. Thinking ahead this would be useful for more complex tasks than just standard cruise control, for example this could represent deviation from a known trajectory (e.g. distance behind the car in front). Here the mass \\(m_0\\) is constant and the control \\(u\\) represents the total thrust. Written in matrix form this corresponds to</p> \\[ \\dot{x} = F x + G u, \\] <p>where</p> \\[ x = \\begin{bmatrix} s \\\\ v \\end{bmatrix}, \\quad F = \\begin{bmatrix} 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix}, \\quad G = \\begin{bmatrix} 0 \\\\ 1/m_0 \\end{bmatrix}. \\] <p>Verify that the more general model from the homework simplifies to this case when the control variable is the thrust, steering control \\(\\delta w = 0\\) and the mass is constant.</p> <p>Let's write down the open loop transfer for this case. Recall that</p> \\[ T_o(s) = H (sI - F)^{-1} G = \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} s &amp; -1 \\\\ 0 &amp; s \\end{bmatrix}^{-1} \\begin{bmatrix} 0 \\\\ 1/m_0 \\end{bmatrix} = \\frac{1}{s^2} \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} s &amp; 1 \\\\ 0 &amp; s \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1/m_0 \\end{bmatrix} = \\frac{1}{m_0 s^2} = N(s)/D(s), \\] <p>where \\(N(s) = 1/m_0\\) and \\(D(s) = s^2\\) in terms of the notation in the previous lecture.</p> <p>For example if our control is \\(u = 4\\) m/s, with the corresponding Laplace transform \\(U(s) = 4/s\\), then</p> \\[ Y(s) = T_0(s) U(s) = \\frac{4}{m_0 s^3}. \\] <p>Assuming zero initial conditions and taking the inverse Laplace Transform gives</p> \\[ y(t) = s(t) = \\frac{2}{m_0} t^2. \\] <p>We could have of course simply integrated the original equation \\(\\ddot{s} = u/m_0\\) twice to obtain the same result. Before writing down a control system for this model we note that the open-loop control has one undesirable characteristic, lack of asymptotic stability. For a constant coefficient linear system this is characterized by not all of the poles of the transfer function having negative real parts (in the above case there is a pole at \\(s=0\\) of multiplicity 2). For precise mathematical definitions of asymptotic stability refer to ODE Notes, or any dynamical systems textbook. Intuitively asymptotic stability of a critical point means that without the nonhomogeneous forcing term (\\(u\\) in our case) the solution will converge to the critical point (\\([0, 0]\\) in our case). For linear constant coefficient systems this is simply the consequence of the solution having the form</p> \\[ x(t) = e^{F(t - t_0)} x_0 + \\int_{t_0}^{t} e^{F(t-s)} G(s) u(s) ds \\] <p>where one can show from the definition of the matrix exponential that for any \\(\\epsilon &gt; 0\\), there exists a positive constant \\(M\\) such that</p> \\[ |e^{Ft} \\zeta| \\le M e^{(b + \\epsilon) t} |\\zeta|, \\] <p>for all \\(\\zeta \\in \\mathbb{R}^n\\), \\(t \\ge 0\\), where \\(b = \\max_{1 \\le i \\le n} \\{ Re(\\lambda_i)\\}\\) and \\(\\lambda_i\\) are the eigenvalues of \\(F\\). Stated more simply the operator norm of the matrix exponential satisfies</p> \\[ ||e^{Ft}|| \\le M e^{(b + \\epsilon) t}. \\] <p>Hence if all eigenvalues are negative and we have appropriate bounds on the control we can always find an \\(\\epsilon\\) to show that the norm of \\(x(t)\\) converges to \\(0\\) as \\(t \\rightarrow \\infty\\).</p> <p>The lack of asymptotic stability has an unfortunate consequence that even a minuscule initial velocity or a very short duration application of the control could have the effect of taking us very far from the starting position as time moves forward (or from a nominal trajectory, like following a car). You can independently verify this for our example by solving the original equation with nonzero initial conditions and/or the nonhomogeneous term \\(u\\) in a piecewise defined form</p> \\[ u = \\begin{cases} u_0 &amp; \\mbox{ if } 0 \\le t \\le t_0, \\\\ 0 &amp; \\mbox{ if } t &gt; t_0. \\end{cases} \\] <p>Of course we did not include friction in the simplified model. The addition of a damping term would have a positive effect of making even the open-loop control stable. In either case, the benefit of adding a closed loop control is that it can potentially stabilize an unstable system or improve stability characteristics of a stable one (note that the convergence in the exponential bound above will improve the more negative we make the real parts of the eigenvalues).</p> <p>Let's return the the transfer function</p> \\[ T_o(s) = \\frac{N(s)}{D(s)} = \\frac{1/m_0}{s^2} \\] <p>and come up with a tracking controller with a slightly more general form</p> \\[ u(t) = k_p e(t) + k_i \\int_0^t e(t) dt + k_d e'(t), \\] <p>where \\(e(t) = y_d(t) - y(t)\\) is the tracking error (see Week 4 Lecture 1). This is called a PID (Proportional, Integral, Derivative) controller. Note the addition of two constants \\(k_i\\) and \\(k_d\\). Mathematically this will give more parameters in the resulting closed-loop transfer function to improve stability. Intuitively the control takes into account not just the current error, but how fast it is changing and the time integral of the error. The last term can be important for example if the error is small and is decreasing, but not decreasing fast enough (think of the integral of \\(1/t\\) vs \\(1/t^2\\) as \\(t \\rightarrow \\infty\\)). In the frequency domain</p> \\[ U(s) = C(s) E(s) = (k_p + k_i/s + k_d s) E(s) = \\frac{k_p s + k_i + k_d s^2}{s} E(s) \\] <p>and as before we may calculate</p> \\[ Y(s) = T_o(s) U(s) = C(s) T_o(s) (Y_d(s) - Y(s)) \\] <p>and solving for \\(Y(s)\\),</p> \\[ Y(s) = \\frac{C(s) T_o(s)}{1 + C(s) T_o(s)} Y_d(s). \\] <p>So in general the closed-loop transfer function is</p> \\[ T_c(s) = \\frac{C(s) T_o(s)}{1 + C(s) T_o(s)} = \\frac{C(s) N(s)}{D(s) + C(s) N(s)}. \\] <p>For the PID control the transfer function becomes</p> \\[ T_c(s) = \\frac{(k_p s + k_i + k_d s^2) T_o(s)}{s + (k_p s + k_i + k_d s^2) T_o(s)} = \\frac{(k_p s + k_i + k_d s^2) N(s)}{s D(s) + (k_p s + k_i + k_d s^2) N(s)} \\] <p>We note several facts. In the limit as \\(k_p\\), \\(k_d\\), and \\(k_i\\) approach \\(0\\) the nonzero poles of \\(T_c\\) converge to the poles of \\(T_o\\). In the limit as \\(k_p, k_d, k_i\\) approach \\(\\pm \\infty\\), \\(m\\) of the poles of \\(T_c\\) converge the \\(m\\) zeros of \\(T_o\\), with \\(T_c\\) potentially having more poles (counting according to multiplicity), which have to go to infinity.</p> <p>For our 1D car example</p> \\[ T_c(s) = \\frac{k_p s + k_i + k_d s^2}{m_0 s^3 + k_p s + k_i + k_d s^2}. \\] <p>We end with a particularly useful feature of the integral control (i.e. \\(k_i \\ne 0\\)). Not only does this term ensure that there are no poles at the origin, but in general it is the property of Laplace Transforms that the steady-state error \\(\\lim_{t\\rightarrow \\infty} (y_d(t) - y(t))\\) is given by</p> \\[ e_{ss} = \\lim_{s \\rightarrow 0} s (Y_d(s) - Y(s)) = \\lim_{s \\rightarrow 0}{ s (1 - T_c(s)) Y_d(s)}. \\] <p>In particular if \\(Y_d(s) = c/s\\) as is the case for the step input it is enough to have \\(T_c \\rightarrow 1\\) as \\(s \\rightarrow 0\\) to ensure zero steady-state error. This is guaranteed for the PID control if \\(k_i \\ne 0\\). The idea is to tune \\(k_p, k_i\\), and \\(k_d\\) to 'place' the poles of the transfer function at the desired location. The difficulty is of course that we now have three parameters instead of one.</p>   Let's do the following exercise. To simplify the situation let $k_d = 0$ and $k_p = 1$ and write the transfer function as  $$ T_c(s) = \\frac{s + k_i}{m_0 s^3 + s + k_i}. $$  - Use the cubic formula to write down expressions for the three roots of the denominator in terms of $k_i$. Use MATLAB or other plotting program/library to plot the root locus (do not use the MATLAB root locus functions). - Consider the full car model with steering. Let $x_l(t) = [x_l(t), y_l(t)]^T$ be a given vector function of $t$ that describes a trajectory of a different car, which we'll call the lead car. Let the horizontal position $x(t)$ be the output of our model. Assume we want to follow this car at a horizontal distance of $10$m. In other words we want the tracking error $e(t) = x(t) - (x_l(t)-10)$ to be small. Derive the closed-loop PID tracking control transfer function for our car model with fixed steering angle, the total thrust $T$ as the control and the $x$ location as the output. Note: We'll assume a different mechanism is responsible for the steering (for example some machine learning based object detection algorithms) and the steering is slow enough that we can take it to be constant for the purposes of our control.   <p>You can check your work using the interactive Cloud Control Toolbox:</p> <p> Open in New Tab </p>"},{"location":"module1/project/","title":"Project 1 - Adaptive Cruise Control","text":"<p>Coming soon...</p>"},{"location":"module1/root-locus-analysis/","title":"Root Locus Analysis","text":"<p>Mathematical connections: complex analysis, asymptotic analysis.</p> <p>We ended the last section with an open loop transfer function</p> \\[ T_o(s) = \\frac{N(s)}{D(s)}, \\] <p>and a closed loop transfer function</p> \\[ T_c(s) = \\frac{k N(s)}{D(s) + kN(s)} \\] <p>that was obtained with proportional control. The poles of \\(T_c\\) correspond to the roots of \\begin{equation} \\label{zeros} D(s) + k N(s) = 0, \\end{equation} where we can assume without loss of generality that \\(D(s)\\) is a polynomial of degree \\(n\\) with leading coefficient \\(1\\), \\(N(s)\\) is a polynomial of degree \\(m\\), \\(m \\le n\\), and \\(N(s)\\), \\(D(s)\\) have no common factors. The method described below due to Evans (..) sheds some light on how these roots depend on the parameter \\(k\\). Clearly as \\(k \\rightarrow 0\\) the poles of \\(T_c\\) approach the zeros of \\(D(s)\\), which are also the poles of \\(T_o\\). Rewriting the equation in the form</p> \\[ N(s) + \\frac{1}{k} D(s) = 0, \\] <p>we note that as \\(k \\rightarrow \\pm\\infty\\) any root that stays bounded must approach a zero of \\(N(s)\\).</p> <p>Let us rewrite the equation one more time as \\begin{equation} \\label{rloc} k \\frac{N(s)}{D(s)} = -1. \\end{equation} This is of course true only if \\(N(s) \\ne 0\\), but this is clearly the case as otherwise \\eqref{zeros} would imply that \\(D(s) = 0\\), implying that both \\(N(s)\\) and \\(D(s)\\) have a common factor \\(s\\). There are several rules which must be satisfied that can help in sketching the poles as \\(k\\) is varied.</p> <p>Let \\(s\\) be such a pole. First, note that since we assume that the coefficients are real, any roots that have an imaginary part must come in complex conjugate pairs. In other words, the root locus picture is symmetric about the real axis. Let \\(z_1, \\dots, z_m\\) be the roots of \\(N(s)\\) (i.e. zeros of \\(T_o\\)) and \\(p_1, \\dots, p_n\\) be the roots of \\(D(s) = 0\\) (i.e. the poles of \\(T_o\\)). Then</p> \\[ k \\frac{N(s)}{D(s)} = k \\frac{b_0 (s - z_1) \\dots (s-z_m)}{(s-p_1) \\dots (s-p_n)} = -1. \\] <p>Note that for real \\(s\\) and any two zeros or poles that are complex conjugate pairs we have</p> \\[ (s - z) (s-\\bar{z}) = (s-z) (\\overline{s-z}) = r e^{i \\phi} r e^{-i \\phi} = r^2 \\ge 0, \\] <p>where \\(s-z = r e^{i \\phi}\\) uses the Euler representation of a complex number.</p> <p>Hence, in order to have a negative product, if \\(s\\) is a real pole of \\(T_c\\), then if \\(k &gt; 0\\) there must be an odd number of real zeros or poles of \\(T_o\\) to the right of \\(s\\). Similarly, if \\(k &lt; 0\\) there must be an even number of real zeros or poles of \\(T_o\\) to the right of \\(s\\).</p> <p>Also note that if for some value of \\(k\\) a pole or a zero is located on the real axis, the only way for it to 'break-away' from the real axis is to split into two branches of the root locus to preserve the symmetry about the real axis. Due to continuity properties of the roots such breakaway points can be created only when two real roots collide (or start out as a higher multiplicity root).</p> <p>How can these rules be used? The analysis assumes that we can plot the zeros and poles of the original open-loop transfer function \\(T_o\\). They are customarily marked with an o and an x respectively on the complex plain. Consider various situations.</p> <ul> <li>If the number of poles and zeros of \\(T_o\\) is the same they should be connected without breaking the symmetry of conjugate roots.</li> <li>If \\(T_o\\) has one pole and no zeros on the real axis, there must be a branch of the root locus covering the whole region to the left of the pole (for \\(k &gt; 0\\)) or to the right of the pole (for \\(k &lt; 0\\)) 'converging' to \\(+\\infty\\) or \\(-\\infty\\) respectively.</li> <li>Similarly, if the original open-loop transfer function has exactly one pole and one zero next to each other, they are either connected by a branch of the root locus or there cannot be a branch of the root locus between them (based on Rule 1 and dependent on the sign of \\(k\\)).</li> <li>Any pole of \\(T_o\\) that doesn't have a zero of \\(T_o\\) to 'go to' must diverge to infinity.</li> </ul> <p>There are other considerations one might consider when trying to draw a root locus by hand, however, with easy access to computational methods we can mostly rely on computers to plot the root locus in real time as we vary \\(k\\) or other parameters. We will develop code to do this in the next few lectures.</p> <p>We end this section with a slightly more technical argument that helps us understand the angle of the asymptotes of the poles diverging to infinity.</p> <p>Let us write \\eqref{rloc} as</p> \\[ 1 +  k \\frac{b_0 (s - z_1) \\dots (s-z_m)}{(s-p_1) \\dots (s-p_n)} = 0. \\] <p>Multiplying out the numerator and the denominator of the rational function above we write it in the expanded form</p> \\[ 1 + k b_0 \\frac{s^m - s^{m-1} \\sum p_i + o(s^{m-1})}{s^n - s^{n-1} \\sum z_i + o(s^{n-1})} = 0 \\] <p>Multiplying both sides by \\(s^{n-m}\\),</p> \\[ s^{n-m} + k b_0 \\frac{s^n - s^{n-1} \\sum p_i + o(s^{n-1})}{s^n - s^{n-1} \\sum z_i + o(s^{n-1})} = 0 \\] <p>and dividing the numerator and denominator of the second term by \\(s^n\\) yields,</p> \\[ s^{n-m} + k b_0 \\frac{1 - \\frac{1}{s} \\sum p_i + o(1/s)}{1 - \\frac{1}{s} \\sum z_i + o(1/s)} = 0 \\] <p>Using the geometric series formula \\(\\frac{1}{1 - t} = 1 + t + o(t)\\) (alternatively the Taylor expansion), we write</p> \\[ \\frac{1}{1 - \\frac{1}{s} \\sum z_i + o(1/s)}  = 1 + \\frac{1}{s} \\sum z_i + o(1/s), \\] <p>and obtain</p> \\[ s^{n-m} + k b_0 \\left(1 - \\frac{1}{s} \\sum p_i + o(1/s) \\right) \\left( 1 + \\frac{1}{s} \\sum z_i + o(1/s) \\right) + o(1/s). \\] <p>or multiplying out the leading terms</p> \\[ s^{n-m} + k b_0 \\left( 1 + \\frac{1}{s} \\left( \\sum z_i - \\sum p_i \\right) + o(1/s)  \\right) = 0. \\] <p>Clearly the different terms have varied behavior as the magnitude of \\(s\\) diverges to \\(\\infty\\). This is the key idea of asymptotic analysis.</p> <p>Let's, multiply the equation by \\(s\\)</p> \\[ s^{n-m+1} + k b_0 \\left( s + \\left( \\sum z_i - \\sum p_i \\right) + o(1)  \\right)  = 0 \\] <p>and introduce the decomposition \\(s = s_0 + r e^{i \\phi}\\). We will see if we can choose \\(s_0\\) in such a way that \\(\\phi\\) stays constant in the limit.</p> <p>Using the binomial theorem,</p> \\[ s^{n-m+1} = (r e^{i \\phi} + s_0)^{n-m+1} = r^{n-m+1} e^{(n-m+1) i \\phi} + (n-m +1) r^{n - m} e^{i (n-m) \\phi} s_0 + o(r^{n-m - 1}) \\] <p>which must equal</p> \\[ - k b_0 \\left( s_0 + r e^{i \\phi} + \\left( \\sum z_i - \\sum p_i \\right) + o(1)  \\right) \\] <p>Dividing both sides by \\(r^{n-m+1}\\) yields our final asymptotic equation</p> \\[ e^{(n-m+1) i \\phi} + \\frac{n-m +1}{r} e^{i (n-m) \\phi} s_0 + o(1/r) = -\\frac{k}{r^{n-m}} b_0 e^{i \\phi} - \\frac{k}{r^{n-m+1}} b_0  \\left( s_0 + \\left( \\sum z_i - \\sum p_i \\right)  \\right) + o\\left(\\frac{k}{r^{n-m+1}}\\right). \\] <p>For this to be true as \\(k,r \\rightarrow \\infty\\), it must hold that</p> \\[ e^{(n-m+1) i \\phi}  \\rightarrow -\\frac{k b_0}{r^{n-m}} e^{i \\phi} \\] <p>and consequently</p> \\[ \\frac{n-m +1}{r} e^{i (n-m) \\phi} s_0 + o(1/r) \\rightarrow - \\frac{k}{r^{n-m+1}} b_0  \\left( s_0 + \\left( \\sum z_i - \\sum p_i \\right)  \\right) \\] <p>These equations imply that as \\(k \\rightarrow \\infty\\),</p> \\[ \\phi \\rightarrow \\frac{l \\pi}{n-m}, \\] <p>where \\(l\\) is odd if \\(k \\ge 0\\) and even if \\(k \\le 0\\), and</p> \\[ (n-m+1) s_0 \\rightarrow \\left( s_0 + \\left( \\sum z_i - \\sum p_i \\right)  \\right) \\] <p>resulting in</p> \\[ s_0 \\rightarrow \\frac{\\sum z_i - \\sum p_i}{n-m} \\] <p>The last expression being necessarily real, we deduce that the asymptotes radiate out of</p> \\[ \\frac{\\sum z_i - \\sum p_i}{n-m} \\] <p>on the real axis with the angle of an odd or even multiple of</p> \\[ \\frac{\\pi}{n-m}, \\] <p>for \\(k &gt; 0\\) and \\(k &lt; 0\\) respectively.</p>"},{"location":"module1/simulating-the-world/","title":"Extra: Simulating the World","text":"<p>Mathematical connections: numerical analysis, systems of linear differential equations</p> <p>Before spending too much time on designing fancy control systems, it would be wise to come up with a plan to test what we do. This is where simulations come into play.</p> <p>Recall that our system is described by a set of differential equations, which for the simpler case of no disturbance and measurement error would look like this:</p> \\[\\begin{equation} \\label{nonlin} \\dot{x}(t) = f(x(t), u(t), t) \\end{equation}\\] <p>and an output equation</p> \\[ y(t) = h(x(t)), \\] <p>where \\(u\\) is the control which we will specify in some way, \\(x\\) is the resulting state, which is nothing other than the solution to the resulting differential equation, and \\(y\\) is the output that we are interested in which in some way depends on the state \\(x\\).</p> <p>In particular, we often deal with linear equations of the form</p> \\[ \\dot{x}(t) = F(t) x(t) + G(t) u(t), \\] \\[ y(t) = H(t) x(t). \\] <p>Once you've designed your controller and specified how to chose the control \\(u(t)\\) you'd naturally want to see what the output \\(y(t)\\) is for such control. This is involves nothing other than solving the differential equations. Depending on the equation there is a very good chance that we would have to do that numerically (either because no analytic solution can be written or even if we have the solution evaluating it would require numerical computations). We'll discuss two methods to numerically solve systems of differential equations. The first, referred to as Runge-Kutta integration can be applied to either of the systems above, while the second will use the extra structure afforded by linearity and naturally will be applied to the linear system (although using the concept of linearization it can be adapted for nonlinear systems as well).</p>"},{"location":"module1/simulating-the-world/#runge-kutta-method","title":"Runge-Kutta Method","text":"<p>To see how the Runge-Kutta method works, we use the Fundamental Theorem of Calculus to rewrite \\eqref{nonlin} as</p> \\[ x(t_k) = x(t_{k-1}) + \\int_{t_{k-1}}^{t_k} f(x(s), u(s), s) ds. \\] <p>where \\(t_{k-1}\\) and \\(t_k\\) are any two values of \\(t\\). The reason for introducing fancy indices is to iterate this process using the fact that we know the initial condition \\(x(t_0)\\). There is only one problem with this. The expression under the integral sign above depends on all of the time values between \\(t_{k-1}\\) and \\(t_k\\), so we are left with trying to approximate this value. The fourth order Runge-Kutta algorithm does this in the following way:</p>   $$ x(t_k) = x(t_{k-1}) + \\frac{1}{6} (\\Delta x_1 + 2 \\Delta x_2 + 2 \\Delta x_3 + \\Delta x_4), $$  where  $$ \\Delta x_1 = f(x(t_{k-1}), u(t_{k-1})) \\Delta t, $$  $$ \\Delta x_2 = f\\left(x(t_{k-1}) + \\Delta x_1/2, u(t_{k-1/2})\\right) \\Delta t, $$  $$ \\Delta x_3 = f\\left(x(t_{k-1}) + \\Delta x_2/2, u(t_{k-1/2})\\right) \\Delta t, $$  $$ \\Delta x_4 = f\\left(x(t_{k-1}) + \\Delta x_3, u(t_{k})\\right) \\Delta t. $$   <p>Here \\(\\Delta t = t_k - t_{k-1}\\) is typically chosen uniform for all values of \\(k\\), but does not have to. Note that we are approximating \\(x\\) based on the previous known value \\(x(t_{k-1})\\), while for \\(u\\) we are using the value in the middle of the interval \\(u(t_{k-1/2})\\) as well. If control is closed loop, we would use the same type of approximation for \\(u\\) as we do for \\(x\\). If you are interested to learn more (including why this works) check out any textbook on numerical methods. At this point it would be great practice to solve a simple scalar differential equation like \\(\\dot{x} = x^2, x(0) = 1\\) using this method. How does it compare with the correct solution as you make \\(\\Delta t\\) smaller?</p> <p>Great, you now have a powerful tool solving differential equations. You can choose some way to visualize the state \\(x\\) at each step of your iterative numerical algorithm and you have yourself a real-world simulation!</p>"},{"location":"module1/simulating-the-world/#matrix-exponential-method-for-linear-systems","title":"Matrix Exponential Method for Linear Systems","text":"<p>We'll finish this lesson with a different approach that is suited for linear equations. These have a lot of extra structure that in some cases allow explicit formulas for the solutions. With such a system we associated an invertible fundamental matrix \\(\\Phi(t)\\), whose columns consist of solutions to the homogeneous differential equation</p> \\[ \\dot{x}(t) = F(t) x(t). \\] <p>Such \\(\\Phi\\) is not unique. At this point you might want to take a look at the notes, Linear Systems, for a more detailed description of the theory behind linear systems.</p> <p>Using properties of matrix multiplication this is equivalent to saying that \\(\\Phi\\) satisfies the matrix equation</p> \\[ \\dot{\\Phi}(t) = F(t) \\Phi(t), \\] <p>where the dot notation implies differentiation of each element of the matrix. It can be shown that such a matrix allows us to write the solution to</p> \\[ \\dot{x}(t) = F(t) x(t), \\quad x(t_0) = x_0, \\] <p>simply as</p> \\[ x(t) = \\Phi(t) \\Phi^{-1}(t_0) x_0. \\] <p>Again anticipating an iterative procedure we can rename the states to</p> \\[ x(t_{k}) = \\Phi(t_k) \\Phi^{-1}(t_{k-1}) x(t_{k-1}) = U(t_{k-1}, t_k) x(t_{k-1}), \\] <p>where the matrix \\(U(t_{k-1}, t_k) = \\Phi(t_k) \\Phi^{-1}(t_{k-1})\\) is called a state transition matrix since multiplication by it maps the state at \\(t_{k-1}\\) to \\(t_k\\). This is great! If we know a fundamental matrix \\(\\Phi\\) we can apply this iterative process starting at \\(t_0\\) where we know the initial condition. If the equation is not homogeneous, i.e. we are solving</p> \\[ \\dot{x}(t) = F(t) x(t) + e(t), \\] <p>one can show relatively easily that the corresponding solution is</p> \\[ x(t) = \\Phi(t) \\Phi^{-1}(t_0) x_0 + \\int_{t_0}^t \\Phi(t) \\Phi^{-1}(s) e(s) ds, \\] <p>or for iterative purposes</p> \\[ x(t_k) = \\Phi(t_k) \\Phi^{-1}(t_{k-1}) x(t_{k-1}) + \\int_{t_{k-1}}^{t_k} \\Phi(t_k) \\Phi^{-1}(s) e(s) ds. \\] <p>This is the famous 'Variation of Parameters' formula for the solution of nonhomogeneous system. For open loop control \\(e(s) = G(s) u(s)\\) and so the solution formula becomes:</p> \\[ x(t_k) = \\Phi(t_k) \\Phi^{-1}(t_{k-1}) x(t_{k-1}) + \\int_{t_{k-1}}^{t_k} \\Phi(t_k) \\Phi^{-1}(s) G(s) u(s) ds. \\] <p>This is all great, but there is one problem, we need to find this fundamental matrix \\(\\Phi\\). Unfortunately in most cases this is not possible. Luckily for us things become simpler if \\(F\\) is constant. For the resulting constant coefficient system a fundamental matrix is expressed in terms of a matrix exponential,</p> \\[ \\Phi(t) = e^{F t} \\] <p>which can be calculated based on the eigenvalues of \\(F\\). This particular choice of \\(\\Phi\\) also has the property that \\(\\Phi(0) = I\\). See any differential equations text for more on how to calculate this. The definition of the matrix exponential is based on the Taylor series expansion of the exponential function and luckily for us shares many properties with its scalar counterpart. In particular</p> \\[ \\Phi^{-1}(t) = e^{-F t}, \\] <p>and</p> \\[ \\Phi(t) \\Phi^{-1}(s) = e^{F(t-s)}. \\] <p>It follows that</p> \\[ x(t_k) = \\Phi(t_k) \\Phi^{-1}(t_{k-1}) x(t_{k-1}) + \\int_{t_{k-1}}^{t_k} \\Phi(t_k) \\Phi^{-1}(s) G(s) u(s) ds \\] \\[ = e^{F(t_k - t_{k-1})} x(t_{k-1}) + \\int_{t_{k-1}}^{t_k} e^{F(t_k-s)} G(s) u(s) ds \\] <p>If, in addition \\(u\\) and \\(G\\) are constant on the interval (this will often be the case in our control systems)</p> \\[ x(t_k) = e^{F(t_k - t_{k-1})} x(t_{k-1}) + \\left( \\int_{t_{k-1}}^{t_k} e^{F(t_k-s)} ds \\right) G u(t_{k-1}). \\] <p>Don't forget that the integral above is actually a matrix that is then multiplied by \\(G u\\). Finally we can change variables in the integral \\(\\tau = s - t_{k-1}\\) to obtain</p> \\[ x(t_k) = e^{F(t_k - t_{k-1})} x(t_{k-1}) + \\left( \\int_{0}^{t_k-t_{k-1}} e^{F(t_k-t_{k-1}-\\tau)} d\\tau \\right) G u(t_{k-1}). \\] \\[ x(t_k) = e^{F(\\Delta t)} x(t_{k-1}) + e^{F(\\Delta t)} \\left( \\int_{0}^{\\Delta t} e^{-F \\tau} d\\tau \\right) G u(t_{k-1}). \\] <p>The final step is to note that</p> \\[ \\int_{0}^{\\Delta t} e^{-F \\tau} d\\tau = -[ e^{-F(\\Delta t)} - I ] F^{-1}. \\] <p>Here \\(I\\) is the identity matrix and this formula a consequence of the definition of the matrix exponential (note that if \\(F\\) was a scalar instead of a matrix we would get the usual integration formula for the exponential).</p> <p>Our final solution is then:</p> \\[ x(t_k) = e^{F(\\Delta t)} x(t_{k-1}) + e^{F(\\Delta t)} [ I - e^{-F(\\Delta t)} ] F^{-1}G u(t_{k-1}). \\] <p>or combining the exponentials</p>   $$ x(t_k) = e^{F(\\Delta t)} x(t_{k-1}) + [e^{F(\\Delta t)} - I ] F^{-1}G u(t_{k-1}). $$   <p>Compared to the iteration formula that we obtained with Runge-Kutta algorithm, the above solution is exact even if \\(\\Delta t\\) is large. Of course it is only valid if \\(F\\), \\(G\\), and \\(u\\) are constant on the interval.</p>"},{"location":"module2/calculus-of-variations/","title":"Calculus of Variations","text":"<p>Coming soon...</p>"},{"location":"module2/hjb/","title":"Hamilton-Jacobi-Bellman Equation","text":"<p>Coming soon...</p>"},{"location":"module2/introduction/","title":"Introduction to Optimal Control","text":"<p>Coming soon...</p>"},{"location":"module2/lqr/","title":"Linear Quadratic Regulator","text":"<p>Coming soon...</p>"},{"location":"module2/maximum-principle/","title":"Maximum Principle","text":"<p>Coming soon...</p>"},{"location":"module2/optimal-control-cov/","title":"Optimal Control using Calculus of Variations","text":"<p>Coming soon...</p>"},{"location":"module2/project/","title":"Project 2 - Planetary Landing Guidance","text":"<p>Coming soon...</p>"},{"location":"module3/adaptive-filtering/","title":"Extra: Adaptive Filtering","text":"<p>Coming soon...</p>"},{"location":"module3/gaussian-filters/","title":"Gaussian Filters","text":"<p>Coming soon...</p>"},{"location":"module3/introduction/","title":"Introduction to State Estimation","text":"<p>Coming soon...</p>"},{"location":"module3/particle-filters/","title":"Particle Filters","text":"<p>Coming soon...</p>"},{"location":"module3/simple-estimator/","title":"Simple Estimator","text":"<p>Coming soon...</p>"},{"location":"module4/controllability/","title":"Controllability","text":"<p>Coming soon...</p>"},{"location":"module4/eigenvalue-assignment/","title":"Eigenvalue Assignment","text":"<p>Coming soon...</p>"},{"location":"module4/h2-optimal-control/","title":"H2 Optimal Control","text":"<p>Coming soon...</p>"},{"location":"module4/mrac/","title":"Model Reference Adaptive Control","text":"<p>Coming soon...</p>"},{"location":"module4/observability/","title":"Observability","text":"<p>Coming soon...</p>"},{"location":"module4/robust-adaptive-control/","title":"Robust Adaptive Control","text":"<p>Coming soon...</p>"}]}